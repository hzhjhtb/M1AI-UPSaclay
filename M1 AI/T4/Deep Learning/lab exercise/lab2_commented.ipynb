{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"tags":[]},"source":["Students:\n","\n","- Zhe HUANG\n","- ...\n","- ...\n","\n","# Deep Learning - Lab Exercise 2\n","\n","**WARNING:** you must have finished the first exercise before this one as you will re-use parts of the code.\n","\n","In the first lab exercise, we built a simple linear classifier.\n","Although it can give reasonable results on the MNIST datasetÂ (~92.5% of accuracy), deeper neural networks can achieve more the 99% accuracy.\n","However, it can quickly become really impracical to explicitly code forward and backward passes.\n","Hence, it is useful to rely on an auto-diff library where we specify the forward pass once, and the backward pass is automatically deduced from the computational graph structure.\n","\n","In this lab exercise, we will build a small and simple auto-diff lib that mimics the autograd mechanism from Pytorch (of course, we will simplify a lot!)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:12:48.966009Z","iopub.status.busy":"2023-04-05T10:12:48.965301Z","iopub.status.idle":"2023-04-05T10:12:48.980109Z","shell.execute_reply":"2023-04-05T10:12:48.978975Z","shell.execute_reply.started":"2023-04-05T10:12:48.965968Z"},"tags":[],"trusted":true},"outputs":[],"source":["# import libs that we will use\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","\n","# To load the data we will use the script of Gaetan Marceau Caron\n","# You can download it from the course webiste and move it to the same directory that contains this ipynb file\n","import dataset_loader\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:12:48.983044Z","iopub.status.busy":"2023-04-05T10:12:48.982322Z","iopub.status.idle":"2023-04-05T10:12:48.994939Z","shell.execute_reply":"2023-04-05T10:12:48.993913Z","shell.execute_reply.started":"2023-04-05T10:12:48.983005Z"},"trusted":true},"outputs":[],"source":["np.random.seed(6942042)"]},{"cell_type":"markdown","metadata":{},"source":["# Data"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:12:48.997068Z","iopub.status.busy":"2023-04-05T10:12:48.996558Z","iopub.status.idle":"2023-04-05T10:12:49.011365Z","shell.execute_reply":"2023-04-05T10:12:49.009937Z","shell.execute_reply.started":"2023-04-05T10:12:48.997027Z"},"tags":[],"trusted":true},"outputs":[],"source":["mnist_path = \"./mnist.pkl.gz\""]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:12:49.015311Z","iopub.status.busy":"2023-04-05T10:12:49.014221Z","iopub.status.idle":"2023-04-05T10:12:49.628128Z","shell.execute_reply":"2023-04-05T10:12:49.626706Z","shell.execute_reply.started":"2023-04-05T10:12:49.015246Z"},"tags":[],"trusted":true},"outputs":[],"source":["# load the 3 splits\n","train_data, dev_data, test_data = dataset_loader.load_mnist(mnist_path)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:12:49.637430Z","iopub.status.busy":"2023-04-05T10:12:49.636224Z","iopub.status.idle":"2023-04-05T10:12:49.887929Z","shell.execute_reply":"2023-04-05T10:12:49.886536Z","shell.execute_reply.started":"2023-04-05T10:12:49.637386Z"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["label: 3\n"]},{"data":{"text/plain":["<matplotlib.image.AxesImage at 0x11a08ea70>"]},"execution_count":8,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZsklEQVR4nO3df2hV9/3H8dfV6l1qby4NMbk3Mw2hKB1GhKr1B/4Gg4HKNBuohRE3kNoaIcROlgoz7A8jbop/ZLq1DKetTmFV51BqU2LiSmax1mJwnYsYZzoTUjO9N0Z3xfr5/iHeb6+x2nN7r+/c5PmAC73n3o/33dPTPHt67z3xOeecAAAwMMx6AADA0EWEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAmaesB3jQ3bt3deXKFQUCAfl8PutxAAAeOefU29urgoICDRv26HOdARehK1euqLCw0HoMAMB31NHRoTFjxjzyOQMuQoFAQNK94bOzs42nAQB4FY1GVVhYGP95/ihpi9D27dv161//Wp2dnRo/fry2bdumWbNmPXbd/f8Fl52dTYQAIIN9m7dU0vLBhP3796uqqkrr16/XmTNnNGvWLJWVleny5cvpeDkAQIbypeMq2lOnTtWLL76oHTt2xLf94Ac/0OLFi1VXV/fItdFoVMFgUJFIhDMhAMhAXn6Op/xM6Pbt2zp9+rRKS0sTtpeWlqqlpaXf82OxmKLRaMINADA0pDxCV69e1VdffaX8/PyE7fn5+erq6ur3/Lq6OgWDwfiNT8YBwNCRti+rPviGlHPuoW9S1dTUKBKJxG8dHR3pGgkAMMCk/NNxubm5Gj58eL+znu7u7n5nR5Lk9/vl9/tTPQYAIAOk/Exo5MiRmjRpkhoaGhK2NzQ0aMaMGal+OQBABkvL94Sqq6v1k5/8RJMnT9b06dP11ltv6fLly1q1alU6Xg4AkKHSEqGlS5eqp6dHv/rVr9TZ2amSkhIdPXpURUVF6Xg5AECGSsv3hL4LvicEAJnN9HtCAAB8W0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwEzKI1RbWyufz5dwC4VCqX4ZAMAg8FQ6/tDx48frww8/jN8fPnx4Ol4GAJDh0hKhp556irMfAMBjpeU9oba2NhUUFKi4uFjLli3TxYsXv/G5sVhM0Wg04QYAGBpSHqGpU6dq9+7dOnbsmN5++211dXVpxowZ6unpeejz6+rqFAwG47fCwsJUjwQAGKB8zjmXzhfo6+vT888/r3Xr1qm6urrf47FYTLFYLH4/Go2qsLBQkUhE2dnZ6RwNAJAG0WhUwWDwW/0cT8t7Ql83atQoTZgwQW1tbQ993O/3y+/3p3sMAMAAlPbvCcViMX3++ecKh8PpfikAQIZJeYTeeOMNNTc3q729XR9//LF+/OMfKxqNqqKiItUvBQDIcCn/33FffPGFli9frqtXr2r06NGaNm2aTp48qaKiolS/FAAgw6U8Qvv27Uv1H4kBqrGx0fOan/70p57XfPHFF57X3L171/MaSUmdsW/atMnzGr5HB9zDteMAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNp/6V2GLz+/Oc/e17zn//8x/Man8/nec2wYcn999W7777rec2tW7c8r/n973/vec3w4cM9rwkEAp7XAE8SZ0IAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAww1W0oQsXLiS17uDBgymeJDO99957ntd8/PHHntf85je/8bymtLTU8xpJys7OTmod4BVnQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGZ9zzlkP8XXRaFTBYFCRSISLKD4hzz77bFLrent7UzxJ6iR7WPt8vhRPkjrJ/D299NJLSb1WQ0OD5zXPPPNMUq+FwcfLz3HOhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM09ZD4DUisVintdcv349qddau3at5zW1tbWe11y7ds3zmiNHjnheI0k///nPPa+5efNmUq/lVTIXME324rRcjBRPCmdCAAAzRAgAYMZzhE6cOKFFixapoKBAPp9Phw4dSnjcOafa2loVFBQoKytLc+fO1blz51I1LwBgEPEcob6+Pk2cOFH19fUPfXzz5s3aunWr6uvrderUKYVCIS1YsGBA/wI0AIANzx9MKCsrU1lZ2UMfc85p27ZtWr9+vcrLyyVJu3btUn5+vvbu3atXX331u00LABhUUvqeUHt7u7q6ulRaWhrf5vf7NWfOHLW0tDx0TSwWUzQaTbgBAIaGlEaoq6tLkpSfn5+wPT8/P/7Yg+rq6hQMBuO3wsLCVI4EABjA0vLpOJ/Pl3DfOddv2301NTWKRCLxW0dHRzpGAgAMQCn9smooFJJ074woHA7Ht3d3d/c7O7rP7/fL7/encgwAQIZI6ZlQcXGxQqGQGhoa4ttu376t5uZmzZgxI5UvBQAYBDyfCd24cUMXLlyI329vb9dnn32mnJwcPffcc6qqqtLGjRs1duxYjR07Vhs3btTTTz+tV155JaWDAwAyn+cIffLJJ5o3b178fnV1tSSpoqJCf/zjH7Vu3TrdunVLr7/+uq5du6apU6fqgw8+UCAQSN3UAIBBweeSuSpiGkWjUQWDQUUiEWVnZ1uPk3H27dvnec3PfvazpF7r008/9bzmhRdeSOq1npT29nbPa5K56OmDVxr5NpL5VzUvL8/zGklqbW31vCY3Nzep18Lg4+XnONeOAwCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgJmU/mZVZKZFixYltW6gXxE7GcXFxZ7XvPPOO57XLF++3POav/71r57XfPnll57XSFJvb6/nNVxFG8ngTAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMMMFTAeZJUuWPJE1+H9ZWVme1yxbtszzmmQuYAoMdJwJAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmuIDpIOP3+61HyGidnZ2e19TU1Hhe884773he45zzvAYY6DgTAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMcAFTDEp37txJal1VVZXnNe+9957nNT6fz/OaZCT7OkeOHPG8prKyMqnXwtDGmRAAwAwRAgCY8RyhEydOaNGiRSooKJDP59OhQ4cSHl+xYoV8Pl/Cbdq0aamaFwAwiHiOUF9fnyZOnKj6+vpvfM7ChQvV2dkZvx09evQ7DQkAGJw8fzChrKxMZWVlj3yO3+9XKBRKeigAwNCQlveEmpqalJeXp3HjxmnlypXq7u7+xufGYjFFo9GEGwBgaEh5hMrKyrRnzx41NjZqy5YtOnXqlObPn69YLPbQ59fV1SkYDMZvhYWFqR4JADBApfx7QkuXLo3/dUlJiSZPnqyioiIdOXJE5eXl/Z5fU1Oj6urq+P1oNEqIAGCISPuXVcPhsIqKitTW1vbQx/1+v/x+f7rHAAAMQGn/nlBPT486OjoUDofT/VIAgAzj+Uzoxo0bunDhQvx+e3u7PvvsM+Xk5CgnJ0e1tbX60Y9+pHA4rEuXLunNN99Ubm6ulixZktLBAQCZz3OEPvnkE82bNy9+//77ORUVFdqxY4daW1u1e/duXb9+XeFwWPPmzdP+/fsVCARSNzUAYFDwOeec9RBfF41GFQwGFYlElJ2dbT0OMtQ///nPpNaVlJSkeJLUSeZf1WQvYDp69GjPa1atWuV5zYYNGzyvwcDn5ec4144DAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGa6ijUHp2WefTWpdb29viidJnSd5Fe0n5c6dO9YjIA24ijYAICMQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaesh4ASIdkL2Cak5OT4kkeLhKJeF7z3//+Nw2T2PrXv/7lec24cePSMAmscCYEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJjhAqYYlC5evGg9wiNdvXrV85rt27d7XrNjxw7PayTpyy+/TGqdV59++qnnNVzAdHDhTAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMMMFTAEDubm5ntf88pe/9Lymq6vL8xpJeuutt5Ja59XLL7/8RF4HAxdnQgAAM0QIAGDGU4Tq6uo0ZcoUBQIB5eXlafHixTp//nzCc5xzqq2tVUFBgbKysjR37lydO3cupUMDAAYHTxFqbm7W6tWrdfLkSTU0NOjOnTsqLS1VX19f/DmbN2/W1q1bVV9fr1OnTikUCmnBggXq7e1N+fAAgMzm6YMJ77//fsL9nTt3Ki8vT6dPn9bs2bPlnNO2bdu0fv16lZeXS5J27dql/Px87d27V6+++mrqJgcAZLzv9J5QJBKRJOXk5EiS2tvb1dXVpdLS0vhz/H6/5syZo5aWlof+GbFYTNFoNOEGABgako6Qc07V1dWaOXOmSkpKJP3/x0Hz8/MTnpufn/+NHxWtq6tTMBiM3woLC5MdCQCQYZKOUGVlpc6ePas//elP/R7z+XwJ951z/bbdV1NTo0gkEr91dHQkOxIAIMMk9WXVNWvW6PDhwzpx4oTGjBkT3x4KhSTdOyMKh8Px7d3d3f3Oju7z+/3y+/3JjAEAyHCezoScc6qsrNSBAwfU2Nio4uLihMeLi4sVCoXU0NAQ33b79m01NzdrxowZqZkYADBoeDoTWr16tfbu3au//OUvCgQC8fd5gsGgsrKy5PP5VFVVpY0bN2rs2LEaO3asNm7cqKefflqvvPJKWv4GAACZy1OEduzYIUmaO3duwvadO3dqxYoVkqR169bp1q1bev3113Xt2jVNnTpVH3zwgQKBQEoGBgAMHp4i5Jx77HN8Pp9qa2tVW1ub7EwAhohnnnnGegQY49pxAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMJPUb1YF8OR1d3d7XtPS0pLUa32bK+YDqcCZEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghguYAhkiHA57XuPz+ZJ6rfz8fM9rXnvttaReC0MbZ0IAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkuYAoYuHr1qvUIj/Tmm296XlNZWZmGSTDYcSYEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJjhAqaAgQ8//NDzmiVLlnheU15e7nmNJC1dujSpdYBXnAkBAMwQIQCAGU8Rqqur05QpUxQIBJSXl6fFixfr/PnzCc9ZsWKFfD5fwm3atGkpHRoAMDh4ilBzc7NWr16tkydPqqGhQXfu3FFpaan6+voSnrdw4UJ1dnbGb0ePHk3p0ACAwcHTBxPef//9hPs7d+5UXl6eTp8+rdmzZ8e3+/1+hUKh1EwIABi0vtN7QpFIRJKUk5OTsL2pqUl5eXkaN26cVq5cqe7u7m/8M2KxmKLRaMINADA0JB0h55yqq6s1c+ZMlZSUxLeXlZVpz549amxs1JYtW3Tq1CnNnz9fsVjsoX9OXV2dgsFg/FZYWJjsSACADJP094QqKyt19uxZffTRRwnbv/79gpKSEk2ePFlFRUU6cuTIQ7+zUFNTo+rq6vj9aDRKiABgiEgqQmvWrNHhw4d14sQJjRkz5pHPDYfDKioqUltb20Mf9/v98vv9yYwBAMhwniLknNOaNWt08OBBNTU1qbi4+LFrenp61NHRoXA4nPSQAIDBydN7QqtXr9a7776rvXv3KhAIqKurS11dXbp165Yk6caNG3rjjTf097//XZcuXVJTU5MWLVqk3NzcpC45AgAY3DydCe3YsUOSNHfu3ITtO3fu1IoVKzR8+HC1trZq9+7dun79usLhsObNm6f9+/crEAikbGgAwODg+X/HPUpWVpaOHTv2nQYCAAwdPve4sjxh0WhUwWBQkUhE2dnZ1uMAADzy8nOcC5gCAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABg5inrAR7knJMkRaNR40kAAMm4//P7/s/zRxlwEert7ZUkFRYWGk8CAPguent7FQwGH/kcn/s2qXqC7t69qytXrigQCMjn8yU8Fo1GVVhYqI6ODmVnZxtNaI/9cA/74R72wz3sh3sGwn5wzqm3t1cFBQUaNuzR7/oMuDOhYcOGacyYMY98TnZ29pA+yO5jP9zDfriH/XAP++Ee6/3wuDOg+/hgAgDADBECAJjJqAj5/X5t2LBBfr/fehRT7Id72A/3sB/uYT/ck2n7YcB9MAEAMHRk1JkQAGBwIUIAADNECABghggBAMxkVIS2b9+u4uJife9739OkSZP0t7/9zXqkJ6q2tlY+ny/hFgqFrMdKuxMnTmjRokUqKCiQz+fToUOHEh53zqm2tlYFBQXKysrS3Llzde7cOZth0+hx+2HFihX9jo9p06bZDJsmdXV1mjJligKBgPLy8rR48WKdP38+4TlD4Xj4NvshU46HjInQ/v37VVVVpfXr1+vMmTOaNWuWysrKdPnyZevRnqjx48ers7MzfmttbbUeKe36+vo0ceJE1dfXP/TxzZs3a+vWraqvr9epU6cUCoW0YMGC+HUIB4vH7QdJWrhwYcLxcfTo0Sc4Yfo1Nzdr9erVOnnypBoaGnTnzh2Vlpaqr68v/pyhcDx8m/0gZcjx4DLESy+95FatWpWw7YUXXnC/+MUvjCZ68jZs2OAmTpxoPYYpSe7gwYPx+3fv3nWhUMht2rQpvu1///ufCwaD7ne/+53BhE/Gg/vBOecqKircD3/4Q5N5rHR3dztJrrm52Tk3dI+HB/eDc5lzPGTEmdDt27d1+vRplZaWJmwvLS1VS0uL0VQ22traVFBQoOLiYi1btkwXL160HslUe3u7urq6Eo4Nv9+vOXPmDLljQ5KampqUl5encePGaeXKleru7rYeKa0ikYgkKScnR9LQPR4e3A/3ZcLxkBERunr1qr766ivl5+cnbM/Pz1dXV5fRVE/e1KlTtXv3bh07dkxvv/22urq6NGPGDPX09FiPZub+P/+hfmxIUllZmfbs2aPGxkZt2bJFp06d0vz58xWLxaxHSwvnnKqrqzVz5kyVlJRIGprHw8P2g5Q5x8OAu4r2ozz4qx2cc/22DWZlZWXxv54wYYKmT5+u559/Xrt27VJ1dbXhZPaG+rEhSUuXLo3/dUlJiSZPnqyioiIdOXJE5eXlhpOlR2Vlpc6ePauPPvqo32ND6Xj4pv2QKcdDRpwJ5ebmavjw4f3+S6a7u7vff/EMJaNGjdKECRPU1tZmPYqZ+58O5NjoLxwOq6ioaFAeH2vWrNHhw4d1/PjxhF/9MtSOh2/aDw8zUI+HjIjQyJEjNWnSJDU0NCRsb2ho0IwZM4ymsheLxfT5558rHA5bj2KmuLhYoVAo4di4ffu2mpubh/SxIUk9PT3q6OgYVMeHc06VlZU6cOCAGhsbVVxcnPD4UDkeHrcfHmbAHg+GH4rwZN++fW7EiBHuD3/4g/vHP/7hqqqq3KhRo9ylS5esR3ti1q5d65qamtzFixfdyZMn3csvv+wCgcCg3we9vb3uzJkz7syZM06S27p1qztz5oz797//7ZxzbtOmTS4YDLoDBw641tZWt3z5chcOh100GjWePLUetR96e3vd2rVrXUtLi2tvb3fHjx9306dPd9///vcH1X547bXXXDAYdE1NTa6zszN+u3nzZvw5Q+F4eNx+yKTjIWMi5Jxzv/3tb11RUZEbOXKke/HFFxM+jjgULF261IXDYTdixAhXUFDgysvL3blz56zHSrvjx487Sf1uFRUVzrl7H8vdsGGDC4VCzu/3u9mzZ7vW1lbbodPgUfvh5s2brrS01I0ePdqNGDHCPffcc66iosJdvnzZeuyUetjfvyS3c+fO+HOGwvHwuP2QSccDv8oBAGAmI94TAgAMTkQIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAmf8D4tb7FtHN6qYAAAAASUVORK5CYII=","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["index = 900\n","label = train_data[1][index]\n","picture = train_data[0][index]\n","\n","print(\"label: %i\" % label)\n","plt.imshow(picture.reshape(28,28), cmap='Greys')"]},{"cell_type":"markdown","metadata":{},"source":["# Computation nodes\n","\n","Instead of directly manipulating numpy arrays, we will manipulate abstraction that contains:\n","- a value (i.e. a numpy array)\n","- a bool indicating if we wish to compute the gradient with respect to the value\n","- the gradient with respect to the value\n","- the operation to call during backpropagation\n","\n","There will be two kind of nodes:\n","- Tensor: a generic computation node\n","- Parameter: a computation node that is used to store parameters of the network. Parameters are always leaf nodes, i.e. they cannot be build from other computation nodes.\n","\n","Our implementation of the backward pass will be really simple and incorrect in the general case (i.e. won't work with computation graph with loops).\n","We will just apply the derivative function for a given tensor and then call the ones of its antecedents, recursively.\n","This simple algorithm is good enough for this exercise.\n","\n","Note that a real implementation of backprop will store temporary values during forward that can be used during backward to improve computation speed. We do not do that here."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:12:51.008033Z","iopub.status.busy":"2023-04-05T10:12:51.007124Z","iopub.status.idle":"2023-04-05T10:12:51.024258Z","shell.execute_reply":"2023-04-05T10:12:51.022867Z","shell.execute_reply.started":"2023-04-05T10:12:51.007985Z"},"tags":[],"trusted":true},"outputs":[],"source":["class Tensor:\n","    def __init__(self, data, require_grad=False):\n","        # test type of data: should be np array\n","        if isinstance(data, float) or isinstance(data, int):\n","            data = np.array([data,])\n","        if type(data) != np.ndarray:\n","            raise RuntimeError(\"Input should be a numpy array\")\n","\n","        # store data for this tensor\n","        self.data = data\n","        self.require_grad = require_grad\n","        \n","        # this values should be set to enable autograd!\n","        self.gradient = None\n","        self.d = None\n","        self.backptr = None\n","        \n","    def zero_grad(self):\n","        \"\"\"\n","        Set the gradient of thie tensor to 0\n","        \"\"\"\n","        if self.require_grad:\n","            self.gradient = np.zeros_like(self.data)\n","            \n","    def accumulate_gradient(self, gradient):\n","        \"\"\"\n","        Accumulte gradient for this tensor\n","        \"\"\"\n","        if gradient.shape != self.data.shape:\n","            raise RuntimeError(\"Invalid gradient dimension\")\n","\n","        if self.gradient is None:\n","            self.gradient = np.copy(gradient)\n","        else:\n","            self.gradient += gradient\n","            \n","    def backward(self, g=None):\n","        \"\"\"\n","        The backward pass!\n","        If g != None, then g is the gradient for the current node.\n","        i.e. g will be != None only for the loss output.\n","        \n","        You should call the function stored in self.d with correct arguments,\n","        and then recursively call the backward methods of tensors in the backptr list if:\n","        1. they require a gradient\n","        2. they are of type Tensor: check with isinstance(o, Tensor)\n","        \"\"\"\n","        if not self.require_grad:  # stop right now if this node does not require a gradient\n","            return\n","        \n","        if g is not None:\n","            if isinstance(g, float):\n","                g = np.array([g])\n","            if type(g) != np.ndarray:\n","                raise RuntimeError(\"Gradient should be a numpy array\")\n","            if g.shape != self.data.shape:\n","                raise RuntimeError(\"Gradient of different size than the value!\")\n","                \n","            self.gradient = g\n","        else:\n","            g = self.gradient\n","\n","        self.d(self.backptr, g)\n","        \n","        for node in self.backptr:\n","            if not isinstance(node, Parameter) and node.require_grad:\n","                if isinstance(node, Tensor):\n","                    node.backward()\n","                else:\n","                    raise RuntimeError(\"Node should be a Tensor\")\n","\n","    \n","class Parameter(Tensor):\n","    \"\"\"\n","    This class will be used to store parameters of the network only!\n","    \"\"\"\n","    def __init__(self, data, name=\"unamed\"):\n","        super().__init__(data, require_grad=True)\n","        self.name = name\n","        \n","    def backward(self):\n","        raise RuntimeError(\"You cannot backprop from a Parameter node\")"]},{"cell_type":"markdown","metadata":{},"source":["# Functions\n","\n","Functions manipulate tensors and build the required information for autograd.\n","A function returns a Tensor that should have require_grad = True if at least of the arguments require a gradient."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:12:51.028047Z","iopub.status.busy":"2023-04-05T10:12:51.027191Z","iopub.status.idle":"2023-04-05T10:12:51.048010Z","shell.execute_reply":"2023-04-05T10:12:51.046453Z","shell.execute_reply.started":"2023-04-05T10:12:51.027980Z"},"tags":[],"trusted":true},"outputs":[],"source":["def any_require_grad(l):\n","    \"\"\"\n","    Input:\n","    - l: an iterable (e.g. a list)\n","    Ouput:\n","    - True if any tensor in the input requires a gradient\n","    \"\"\"\n","    return any(t.require_grad for t in l)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:12:51.054201Z","iopub.status.busy":"2023-04-05T10:12:51.053112Z","iopub.status.idle":"2023-04-05T10:12:51.065240Z","shell.execute_reply":"2023-04-05T10:12:51.063736Z","shell.execute_reply.started":"2023-04-05T10:12:51.054146Z"},"tags":[],"trusted":true},"outputs":[],"source":["# Here is an exemple with the ReLU\n","def relu(x):\n","    \"\"\"\n","    Compute the ReLU of Tensor x\n","    Parameters:\n","    - x: a Tensor\n","    Returns:\n","    - a Tensor with the same shape as x    \n","    \"\"\"\n","    v = np.maximum(0, x.data)\n","    \n","    output = Tensor(v, require_grad=x.require_grad)\n","    output.d = backward_relu\n","    output.backptr = [x]\n","    \n","    return output\n","\n","def backward_relu(backptr, g):\n","    \"\"\"\n","    The backward pass for the ReLU\n","    Parameters:\n","    - backptr: a list of tensors that are the inputs of the ReLU\n","    - g: the gradient for the output of the ReLU\n","    \"\"\"    \n","    x, = backptr\n","    \n","    # the gradient is accumulated in the arguments only if required\n","    if x.require_grad:\n","        x.accumulate_gradient(g * (x.data > 0))"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:12:51.067825Z","iopub.status.busy":"2023-04-05T10:12:51.066972Z","iopub.status.idle":"2023-04-05T10:12:51.077916Z","shell.execute_reply":"2023-04-05T10:12:51.076817Z","shell.execute_reply.started":"2023-04-05T10:12:51.067765Z"},"tags":[],"trusted":true},"outputs":[],"source":["def tanh(x):\n","    \"\"\"\n","    Compute the tanh of Tensor x\n","    Parameters:\n","    - x: a Tensor\n","    Returns:\n","    - a Tensor with the same shape as x    \n","    \"\"\"\n","    v = np.tanh(x.data)\n","    \n","    output = Tensor(v, require_grad=x.require_grad)\n","    output.d = backward_tanh\n","    output.backptr = [x]\n","    \n","    return output\n","\n","def backward_tanh(backptr, g):\n","    \"\"\"\n","    The backward pass for the tanh\n","    Parameters:\n","    - backptr: a list of tensors that are the inputs of the tanh\n","    - g: the gradient for the output of the tanh\n","    \"\"\"    \n","    x, = backptr\n","    \n","    # the gradient is accumulated in the arguments only if required\n","    if x.require_grad:\n","        x.accumulate_gradient(g * (1 - np.tanh(x.data)**2))"]},{"cell_type":"markdown","metadata":{},"source":["Next, we implement the affine transform operation.\n","You can reuse the code from the first lab exercise, with one major difference: you have to compute the gradient with respect to x too!"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:12:51.080449Z","iopub.status.busy":"2023-04-05T10:12:51.079591Z","iopub.status.idle":"2023-04-05T10:12:51.097443Z","shell.execute_reply":"2023-04-05T10:12:51.096034Z","shell.execute_reply.started":"2023-04-05T10:12:51.080405Z"},"tags":[],"trusted":true},"outputs":[],"source":["def affine_transform(W, b, x):\n","    \"\"\"\n","    Compute the affine_transform of Tensor x\n","    Parameters:\n","    - W: a Parameter\n","    - b: a Parameter\n","    - x: a Tensor\n","    Returns:\n","    - a Tensor with the same shape as x    \n","    \"\"\"\n","    v = x.data @ W.data.T + b.data\n","    \n","    output = Tensor(v, require_grad=any_require_grad([W, b, x]))\n","    output.d = backward_affine_transform\n","    output.backptr = [W, b, x]\n","    return output\n","\n","def backward_affine_transform(backptr, g):\n","    \"\"\"\n","    The backward pass for the affine_transform\n","    Parameters:\n","    - backptr: a list of tensors that are the inputs of the affine_transform\n","    - g: the gradient for the output of the affine_transform\n","    \"\"\"\n","    W, b, x = backptr\n","    \n","    # the gradient is accumulated in the arguments only if required\n","    if W.require_grad:\n","        W.accumulate_gradient(g.reshape(-1,1) @ x.data.reshape(1,-1))\n","    if b.require_grad:\n","        b.accumulate_gradient(g)\n","    if isinstance(x, Tensor) and x.require_grad:\n","        x.accumulate_gradient(g @ W.data)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:12:51.099866Z","iopub.status.busy":"2023-04-05T10:12:51.099476Z","iopub.status.idle":"2023-04-05T10:12:51.112293Z","shell.execute_reply":"2023-04-05T10:12:51.111264Z","shell.execute_reply.started":"2023-04-05T10:12:51.099828Z"},"tags":[],"trusted":true},"outputs":[],"source":["# we use an underscore because this function does not manipulate tensors:\n","# it is exactly the same as in the previous exercise\n","def _softmax(x):\n","    b = np.max(x)\n","    y = np.exp(x - b)\n","    return y / np.sum(y)\n","\n","def nll(x, gold):\n","    \"\"\"\n","    Compute the nll of Tensor x\n","    Parameters:\n","    - x: a Tensor\n","    - gold: a Tensor(but a single value)\n","    Returns:\n","    - a Tensor with the same shape as x    \n","    \"\"\"\n","    v = -x.data[gold.data[0]] + np.log(np.sum(np.exp(x.data)))\n","    \n","    output = Tensor(v, require_grad=any_require_grad([x, gold]))\n","    output.d = backward_nll\n","    output.backptr = [x, gold]\n","    return output\n","\n","def backward_nll(backptr, g):\n","    \"\"\"\n","    The backward pass for the nll\n","    Parameters:\n","    - backptr: a list of tensors that are the inputs of the nll\n","    - g: the gradient for the output of the nll\n","    \"\"\"\n","    x, gold = backptr\n","    \n","    y = _softmax(x.data)\n","    y[gold.data] -= 1\n","    if x.require_grad:\n","        x.accumulate_gradient(g * y)"]},{"cell_type":"markdown","metadata":{},"source":["# Module\n","\n","Neural networks or parts of neural networks will be stored in Modules.\n","They implement method to retrieve all parameters of the network and subnetwork."]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:12:51.114443Z","iopub.status.busy":"2023-04-05T10:12:51.114023Z","iopub.status.idle":"2023-04-05T10:12:51.125443Z","shell.execute_reply":"2023-04-05T10:12:51.124439Z","shell.execute_reply.started":"2023-04-05T10:12:51.114383Z"},"tags":[],"trusted":true},"outputs":[],"source":["class Module:\n","    def __init__(self):\n","        raise NotImplemented(\"\")\n","        \n","    def parameters(self):\n","        ret = []\n","        for name in dir(self):\n","            o = self.__getattribute__(name)\n","\n","            if type(o) is Parameter:\n","                ret.append(o)\n","            if isinstance(o, Module) or isinstance(o, ModuleList):\n","                ret.extend(o.parameters())\n","        return ret\n","\n","# if you want to store a list of Parameters or Module,\n","# you must store them in a ModuleList instead of a python list,\n","# in order to collect the parameters correctly\n","class ModuleList(list):\n","    def parameters(self):\n","        ret = []\n","        for m in self:\n","            if type(m) is Parameter:\n","                ret.append(m)\n","            elif isinstance(m, Module) or isinstance(m, ModuleList):\n","                ret.extend(m.parameters())\n","        return ret"]},{"cell_type":"markdown","metadata":{},"source":["# Initialization and optimization"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:12:51.126886Z","iopub.status.busy":"2023-04-05T10:12:51.126557Z","iopub.status.idle":"2023-04-05T10:12:51.143704Z","shell.execute_reply":"2023-04-05T10:12:51.141433Z","shell.execute_reply.started":"2023-04-05T10:12:51.126854Z"},"tags":[],"trusted":true},"outputs":[],"source":["def zero_init(b):\n","    b[:] = 0.\n","\n","def glorot_init(W):\n","    W[:] = np.random.uniform(-np.sqrt(6. / (W.shape[0] + W.shape[1])), \n","                             np.sqrt(6. / (W.shape[0] + W.shape[1])), \n","                             W.shape)\n","\n","def kaiming_init(W):\n","    W[:] = np.random.uniform(-np.sqrt(6. / W.shape[1]), \n","                             np.sqrt(6. / W.shape[1]), \n","                             W.shape)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:12:51.147401Z","iopub.status.busy":"2023-04-05T10:12:51.146888Z","iopub.status.idle":"2023-04-05T10:12:51.158888Z","shell.execute_reply":"2023-04-05T10:12:51.157655Z","shell.execute_reply.started":"2023-04-05T10:12:51.147349Z"},"tags":[],"trusted":true},"outputs":[],"source":["# simple gradient descent optimizer(with momentum)\n","class SGD:\n","    def __init__(self, params, lr=0.1, momentum=False, beta = 0.5):\n","        \"\"\"\n","        Parameters:\n","        - params: a list of Parameters of the network\n","        - lr: the learning rate\n","        - momentum: boolean, True means we use momentum\n","        - beta: the momentum coefficient\n","        \"\"\"\n","        self.params = params\n","        self.lr = lr\n","        self.momentum = False\n","        \n","        if self.momentum:\n","            self.beta = beta\n","            for p in self.params:\n","                p.momentum = 0\n","        \n","    def step(self):\n","        for p in self.params:\n","            # if we don't use momentum method, this is a simple gradient descent\n","            if not self.momentum:\n","                p.data[:] = p.data - self.lr * p.gradient\n","            # if we use momentum\n","            else:\n","                p.momentum = self.beta * p.momentum + (1-self.beta) * p.gradient\n","                p.data[:] = p.data - self.lr * p.momentum\n","    \n","    def zero_grad(self):\n","        for p in self.params:\n","            p.zero_grad()"]},{"cell_type":"markdown","metadata":{},"source":["# Networks and training loop\n","\n","We first create a simple linear classifier, similar to the first lab exercise."]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:12:51.160242Z","iopub.status.busy":"2023-04-05T10:12:51.159855Z","iopub.status.idle":"2023-04-05T10:12:51.179483Z","shell.execute_reply":"2023-04-05T10:12:51.177957Z","shell.execute_reply.started":"2023-04-05T10:12:51.160208Z"},"tags":[],"trusted":true},"outputs":[],"source":["class LinearNetwork(Module):\n","    def __init__(self, dim_input, dim_output):\n","        # build the parameters\n","        self.W = Parameter(np.ndarray((dim_output, dim_input)), name=\"W\")\n","        self.b = Parameter(np.ndarray((dim_output,)), name=\"b\")\n","        self.dim_input = dim_input\n","        self.dim_output = dim_output\n","        \n","        self.init_parameters()\n","        \n","    def init_parameters(self):\n","        # init parameters of the network (i.e W and b)\n","        glorot_init(self.W.data)\n","        zero_init(self.b.data)\n","        \n","    def forward(self, x):\n","        return affine_transform(self.W, self.b, x)\n","    \n","    def __str__(self):\n","        return \"Linear(%d, %d)\"%(self.dim_input, self.dim_output)"]},{"cell_type":"markdown","metadata":{},"source":["We will train several neural networks.\n","Therefore, we encapsulate the training loop in a function.\n","\n","**warning**: you have to call optimizer.zero_grad() before each backward pass to reinitialize the gradient of the parameters!"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:12:51.197139Z","iopub.status.busy":"2023-04-05T10:12:51.196770Z","iopub.status.idle":"2023-04-05T10:12:51.215454Z","shell.execute_reply":"2023-04-05T10:12:51.213816Z","shell.execute_reply.started":"2023-04-05T10:12:51.197102Z"},"tags":[],"trusted":true},"outputs":[],"source":["def training_loop(network, optimizer, train_data, dev_data, n_epochs=10, decay=False, decay_rate=0.9):\n","    overall_train_loss = [] # the average loss over the training set\n","    overall_dev_loss = [] # the average loss over the dev set\n","    overall_train_accuracy = [] # the average acuracy over the training set\n","    overall_dev_accuracy = [] # the average acuracy over the dev set\n","\n","    for epoch in range(n_epochs):\n","        print(\"Epoch {}\".format(epoch))\n","        epoch_train_loss = []\n","        epoch_dev_loss = []\n","\n","        ## train the network\n","\n","        trainimages = train_data[0]\n","        trainlabels = train_data[1]\n","        # shuffle the data\n","        shuff = np.random.permutation(len(trainimages))\n","        trainimages = trainimages[shuff]\n","        trainlabels = trainlabels[shuff]\n","        correct = 0\n","        print(\"Training...\")\n","        for i in range(len(trainimages)):\n","            # forward pass\n","            x = Tensor(trainimages[i], require_grad=False)\n","            gold = Tensor(np.array([trainlabels[i],]), require_grad=False)\n","            z = network.forward(x)\n","            \n","            # compute loss\n","            loss = nll(z, gold)\n","            epoch_train_loss.append(loss.data[0])\n","            # count correct predictions\n","            predicted = np.argmax(_softmax(z.data))\n","            if predicted == gold.data[0]:\n","                correct += 1\n","            \n","            # backward pass\n","            loss.backward(g=1.0)\n","            optimizer.step()\n","            optimizer.zero_grad()\n","        \n","        # accuracy is computed as the number of correct predictions over the training set\n","        trainacc = correct / len(train_data[0])\n","        overall_train_accuracy.append(trainacc)\n","        \n","        ## evaluate the network\n","\n","        devimages = dev_data[0]\n","        devlabels = dev_data[1]\n","        correct = 0\n","        print(\"Evaluating...\")\n","        for i in range(len(dev_data[0])):\n","            # forward pass\n","            x = Tensor(devimages[i], require_grad=False)\n","            gold = Tensor(np.array([devlabels[i],]), require_grad=False)\n","            z = network.forward(x)\n","\n","            # compute loss\n","            loss = nll(z, gold)\n","            epoch_dev_loss.append(loss.data[0])\n","            # count correct predictions\n","            predicted = np.argmax(_softmax(z.data))\n","            if predicted == gold.data[0]:\n","                correct += 1\n","\n","        # # accuracy is computed as the number of correct predictions over the dev set                   \n","        devacc = correct / len(dev_data[0])\n","        overall_dev_accuracy.append(devacc)\n","        \n","        mean_epoch_train_loss = np.array(epoch_train_loss).mean()\n","        mean_epoch_dev_loss = np.array(epoch_dev_loss).mean()\n","        overall_train_loss.append(mean_epoch_train_loss)\n","        overall_dev_loss.append(mean_epoch_dev_loss)\n","                                  \n","        print(\"Mean train loss : \", mean_epoch_train_loss)\n","        print(\"Mean dev loss   : \", mean_epoch_dev_loss)\n","        print(\"Train accuracy  : \", trainacc)\n","        print(\"Dev accuracy    : \", devacc)\n","        \n","        if decay:\n","            #check if loss is increasing\n","            if epoch > 0 and overall_train_loss[epoch] > overall_train_loss[epoch-1]:\n","                optimizer.lr = optimizer.lr * decay_rate\n","                print(\"Learning rate decayed to: \", optimizer.lr)\n","                                  \n","                                  \n","    return overall_train_loss, overall_dev_loss, overall_train_accuracy, overall_dev_accuracy"]},{"cell_type":"markdown","metadata":{},"source":["After you finished the linear network, you can move to a deep network!"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:12:51.235926Z","iopub.status.busy":"2023-04-05T10:12:51.235485Z","iopub.status.idle":"2023-04-05T10:12:51.252265Z","shell.execute_reply":"2023-04-05T10:12:51.250455Z","shell.execute_reply.started":"2023-04-05T10:12:51.235889Z"},"tags":[],"trusted":true},"outputs":[],"source":["class ReluLayer(Module):\n","    def __init__(self):\n","        pass\n","    \n","    def forward(self, x):\n","        return relu(x)\n","    \n","    def init_parameters(self):\n","        pass\n","    \n","    def __str__(self):\n","        return \"Relu()\"\n","    \n","class TanhLayer(Module):\n","    def __init__(self):\n","        pass\n","    \n","    def forward(self, x):\n","        return tanh(x)\n","    \n","    def init_parameters(self):\n","        pass\n","    \n","    def __str__(self):\n","        return \"Tanh()\""]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:12:51.254400Z","iopub.status.busy":"2023-04-05T10:12:51.254040Z","iopub.status.idle":"2023-04-05T10:12:51.267117Z","shell.execute_reply":"2023-04-05T10:12:51.265804Z","shell.execute_reply.started":"2023-04-05T10:12:51.254367Z"},"tags":[],"trusted":true},"outputs":[],"source":["class DeepNetwork(Module):\n","    def __init__(self, dim_input, dim_output, hidden_dim, n_layers, tanh=False):\n","        self.layers = ModuleList()\n","        \n","        indim = dim_input\n","        outdim = hidden_dim\n","        \n","        for i in range(n_layers):\n","            if i == n_layers-1:\n","                outdim = dim_output\n","            if i > 0:\n","                indim = hidden_dim\n","            self.layers.extend([\n","                LinearNetwork(indim, outdim),\n","                ReluLayer() if not tanh else TanhLayer() # use either Relu either Tanh as activation layer\n","            ])\n","            \n","        # for last linear layer, we don't need the activation layer anymore\n","        self.layers.pop()\n","        \n","        self.init_parameters()\n","        \n","    def init_parameters(self):\n","        for layer in self.layers:\n","            layer.init_parameters()\n","\n","    def forward(self, x):\n","        for l in self.layers:\n","            x = l.forward(x)\n","        return x\n","        \n","    def __str__(self):\n","        ret = \"\"\n","        ret += \"[\\n\"\n","        for l in self.layers:\n","            ret += \"  \" + str(l) + \"\\n\"\n","        ret += \"]\\n\"\n","        return ret"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:12:51.269314Z","iopub.status.busy":"2023-04-05T10:12:51.268862Z","iopub.status.idle":"2023-04-05T10:12:51.289515Z","shell.execute_reply":"2023-04-05T10:12:51.288143Z","shell.execute_reply.started":"2023-04-05T10:12:51.269276Z"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[\n","  Linear(784, 100)\n","  Tanh()\n","  Linear(100, 100)\n","  Tanh()\n","  Linear(100, 10)\n","]\n","\n"]}],"source":["dim_input = 28*28\n","dim_output = 10\n","\n","network = DeepNetwork(dim_input, dim_output, 100, 3, tanh=True)\n","optimizer = SGD(network.parameters(), 0.01, momentum=True)\n","print(network)"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0\n","Training...\n","Evaluating...\n","Mean train loss :  0.25540506442484856\n","Mean dev loss   :  0.13763881123076793\n","Train accuracy  :  0.92328\n","Dev accuracy    :  0.9596\n","Epoch 1\n","Training...\n","Evaluating...\n","Mean train loss :  0.12162485704651575\n","Mean dev loss   :  0.11399796358384058\n","Train accuracy  :  0.96192\n","Dev accuracy    :  0.9655\n","Epoch 2\n","Training...\n","Evaluating...\n","Mean train loss :  0.08496069888943965\n","Mean dev loss   :  0.09781754473653306\n","Train accuracy  :  0.97326\n","Dev accuracy    :  0.9734\n","Epoch 3\n","Training...\n","Evaluating...\n","Mean train loss :  0.06284908132040097\n","Mean dev loss   :  0.09311872109413716\n","Train accuracy  :  0.98002\n","Dev accuracy    :  0.971\n","Epoch 4\n","Training...\n","Evaluating...\n","Mean train loss :  0.048345317295705466\n","Mean dev loss   :  0.0940208469602735\n","Train accuracy  :  0.9846\n","Dev accuracy    :  0.973\n"]},{"data":{"text/plain":["([0.25540506442484856,\n","  0.12162485704651575,\n","  0.08496069888943965,\n","  0.06284908132040097,\n","  0.048345317295705466],\n"," [0.13763881123076793,\n","  0.11399796358384058,\n","  0.09781754473653306,\n","  0.09311872109413716,\n","  0.0940208469602735],\n"," [0.92328, 0.96192, 0.97326, 0.98002, 0.9846],\n"," [0.9596, 0.9655, 0.9734, 0.971, 0.973])"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["training_loop(network, optimizer, train_data, dev_data, n_epochs=5)"]},{"cell_type":"markdown","metadata":{},"source":["## Bonus\n","\n","You can try to implement a momentum SGD optimizer! Note that you have to keep track of the velocity for each parameter in the optimizer.\n"]},{"cell_type":"markdown","metadata":{},"source":["# Experiments"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:12:51.301459Z","iopub.status.busy":"2023-04-05T10:12:51.300325Z","iopub.status.idle":"2023-04-05T10:12:51.313997Z","shell.execute_reply":"2023-04-05T10:12:51.312230Z","shell.execute_reply.started":"2023-04-05T10:12:51.301404Z"},"trusted":true},"outputs":[],"source":["grid_layers = [2, 3, 5]\n","grid_decay = [True, False]\n","grid_momentum = [True, False]\n","grid_tanh = [True, False]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-05T10:12:51.316707Z","iopub.status.busy":"2023-04-05T10:12:51.315848Z","iopub.status.idle":"2023-04-05T10:13:38.801875Z","shell.execute_reply":"2023-04-05T10:13:38.799866Z","shell.execute_reply.started":"2023-04-05T10:12:51.316627Z"},"trusted":true},"outputs":[],"source":["for layersval in grid_layers:\n","    for decayval in grid_decay:\n","        for momentumval in grid_momentum:\n","            for tanhval in grid_tanh:\n","                print(\"layers=%d decay=%s momentum=%s tanh=%s\"%(layersval, decayval, momentumval, tanhval))\n","                network = DeepNetwork(dim_input, dim_output, 100, layersval, tanh=tanhval)\n","                optimizer = SGD(network.parameters(), 0.01, momentum=momentumval)\n","                tloss, dloss, tacc, dacc = training_loop(network, optimizer, train_data, dev_data, n_epochs=15, decay=decayval)\n","                x = list(range(15))\n","                plt.plot(x, tloss, label=\"Training loss\")\n","                plt.plot(x, dloss, label=\"Dev loss\")\n","                plt.xlabel(\"Epoch\")\n","                plt.ylabel(\"Loss\")\n","                plt.legend()\n","                plt.show()\n","                plt.plot(x, tacc, label=\"Training accuracy\")\n","                plt.plot(x, dacc, label=\"Dev accuracy\")\n","                plt.xlabel(\"Epoch\")\n","                plt.ylabel(\"Accuracy\")\n","                plt.legend()\n","                plt.show()\n","                \n","                "]}],"metadata":{"kernelspec":{"display_name":"MLalgo","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"7d19884719244d884004755aa26b298b23c442cd500cd8cfeba7ff475b14a79e"}}},"nbformat":4,"nbformat_minor":4}
